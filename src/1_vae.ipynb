{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "# CODE FILES HERE\n",
    "from examples.vae.vae import Encoder, Decoder, Vae, PATH\n",
    "from solver import Solver\n",
    "from dataloader import DataLoader\n",
    "from plot import plot_losses, plot_gaussian_distributions, plot_rl_kl, plot_latent_space, plot_latent_space_no_labels, \\\n",
    "plot_latent_manifold, plot_faces_grid, plot_faces_samples_grid\n",
    "\n",
    "%matplotlib inline\n",
    "#plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "#plt.rcParams['image.interpolation'] = 'nearest'\n",
    "#plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# supress cluttering warnings in solutions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the dataset and tune hyperparameters here!\n",
    "dataset = \"FF\"\n",
    "batch_size = 128\n",
    "optimizer = torch.optim.Adam\n",
    "\n",
    "# TODO: set individually for each dataset\n",
    "warmup_epochs = 0\n",
    "beta = 1\n",
    "\n",
    "if dataset == \"MNIST\":\n",
    "    epochs = 100\n",
    "    hidden_dim = 500\n",
    "    z_dim = 20\n",
    "    step_config = {\n",
    "        \"step_size\" : -1,\n",
    "        \"gamma\" : 0.1 # or 0.75\n",
    "    }\n",
    "    optim_config = {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4\n",
    "    }\n",
    "elif dataset == \"LFW\":\n",
    "    epochs = 500\n",
    "    hidden_dim = 700\n",
    "    z_dim = 20\n",
    "    step_config = {\n",
    "        \"step_size\" : 30,\n",
    "        \"gamma\" : 0.1\n",
    "    }\n",
    "    optim_config = {\n",
    "        \"lr\": 1e-1,\n",
    "        \"weight_decay\": 1e-4\n",
    "    }\n",
    "elif dataset == \"FF\":\n",
    "    epochs = 100\n",
    "    hidden_dim = 200\n",
    "    z_dim = 20\n",
    "    step_config = {\n",
    "        \"step_size\" : -1,\n",
    "        \"gamma\" : 0.1\n",
    "    }\n",
    "    optim_config = {\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(PATH, batch_size, dataset, z_dim)\n",
    "encoder = Encoder(data_loader.input_dim, hidden_dim, z_dim)\n",
    "decoder = Decoder(z_dim, hidden_dim, data_loader.input_dim)\n",
    "model = Vae(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ START RUN +++++\n",
      "====> Epoch: 1 train set loss avg: 407.7662\n",
      "====> Test set loss avg: 387.9781\n",
      "0.533930778503418 seconds for epoch 1\n",
      "====> Epoch: 2 train set loss avg: 400.7962\n",
      "====> Test set loss avg: 389.4409\n",
      "0.47655320167541504 seconds for epoch 2\n",
      "====> Epoch: 3 train set loss avg: 396.7981\n",
      "====> Test set loss avg: 392.4545\n",
      "0.4870600700378418 seconds for epoch 3\n",
      "====> Epoch: 4 train set loss avg: 394.1032\n",
      "====> Test set loss avg: 394.9397\n",
      "0.563291072845459 seconds for epoch 4\n",
      "====> Epoch: 5 train set loss avg: 392.3239\n",
      "====> Test set loss avg: 388.3488\n",
      "0.5656392574310303 seconds for epoch 5\n",
      "====> Epoch: 6 train set loss avg: 390.5347\n",
      "====> Test set loss avg: 389.8269\n",
      "0.5661039352416992 seconds for epoch 6\n",
      "====> Epoch: 7 train set loss avg: 389.1505\n",
      "====> Test set loss avg: 388.4996\n",
      "0.4705629348754883 seconds for epoch 7\n",
      "====> Epoch: 8 train set loss avg: 387.7406\n",
      "====> Test set loss avg: 386.9929\n",
      "0.479968786239624 seconds for epoch 8\n",
      "====> Epoch: 9 train set loss avg: 386.4575\n",
      "====> Test set loss avg: 391.0467\n",
      "0.49764537811279297 seconds for epoch 9\n",
      "====> Epoch: 10 train set loss avg: 385.2348\n",
      "====> Test set loss avg: 381.6184\n",
      "0.5814089775085449 seconds for epoch 10\n",
      "====> Epoch: 11 train set loss avg: 384.0574\n",
      "====> Test set loss avg: 383.9720\n",
      "0.5018250942230225 seconds for epoch 11\n",
      "====> Epoch: 12 train set loss avg: 382.9967\n",
      "====> Test set loss avg: 381.4316\n",
      "0.4925093650817871 seconds for epoch 12\n",
      "====> Epoch: 13 train set loss avg: 381.9299\n",
      "====> Test set loss avg: 380.1299\n",
      "0.5093348026275635 seconds for epoch 13\n",
      "====> Epoch: 14 train set loss avg: 380.7014\n",
      "====> Test set loss avg: 385.0116\n",
      "0.5675852298736572 seconds for epoch 14\n",
      "====> Epoch: 15 train set loss avg: 379.7831\n",
      "====> Test set loss avg: 378.0616\n",
      "0.5688228607177734 seconds for epoch 15\n",
      "====> Epoch: 16 train set loss avg: 378.7625\n",
      "====> Test set loss avg: 375.9230\n",
      "0.48975682258605957 seconds for epoch 16\n",
      "====> Epoch: 17 train set loss avg: 377.8615\n",
      "====> Test set loss avg: 378.8727\n",
      "0.49811744689941406 seconds for epoch 17\n",
      "====> Epoch: 18 train set loss avg: 376.8237\n",
      "====> Test set loss avg: 376.3921\n",
      "0.4934415817260742 seconds for epoch 18\n",
      "====> Epoch: 19 train set loss avg: 376.1322\n",
      "====> Test set loss avg: 376.4783\n",
      "0.5125288963317871 seconds for epoch 19\n",
      "====> Epoch: 20 train set loss avg: 375.1081\n",
      "====> Test set loss avg: 375.1097\n",
      "0.5081238746643066 seconds for epoch 20\n",
      "====> Epoch: 21 train set loss avg: 374.4066\n",
      "====> Test set loss avg: 373.0592\n",
      "0.5028622150421143 seconds for epoch 21\n",
      "====> Epoch: 22 train set loss avg: 373.6615\n",
      "====> Test set loss avg: 373.7933\n",
      "0.5000412464141846 seconds for epoch 22\n",
      "====> Epoch: 23 train set loss avg: 372.9156\n",
      "====> Test set loss avg: 372.2564\n",
      "0.5578243732452393 seconds for epoch 23\n",
      "====> Epoch: 24 train set loss avg: 372.0690\n",
      "====> Test set loss avg: 371.8100\n",
      "0.5158798694610596 seconds for epoch 24\n",
      "====> Epoch: 25 train set loss avg: 371.4393\n",
      "====> Test set loss avg: 372.2387\n",
      "0.5223019123077393 seconds for epoch 25\n",
      "====> Epoch: 26 train set loss avg: 370.7072\n",
      "====> Test set loss avg: 370.8985\n",
      "0.51027512550354 seconds for epoch 26\n",
      "====> Epoch: 27 train set loss avg: 370.0528\n",
      "====> Test set loss avg: 370.0460\n",
      "0.5027182102203369 seconds for epoch 27\n",
      "====> Epoch: 28 train set loss avg: 369.3800\n",
      "====> Test set loss avg: 368.9666\n",
      "0.49274611473083496 seconds for epoch 28\n",
      "====> Epoch: 29 train set loss avg: 368.6430\n",
      "====> Test set loss avg: 368.8203\n",
      "0.5065150260925293 seconds for epoch 29\n",
      "====> Epoch: 30 train set loss avg: 368.1699\n",
      "====> Test set loss avg: 367.3721\n",
      "0.5338540077209473 seconds for epoch 30\n",
      "====> Epoch: 31 train set loss avg: 367.6842\n",
      "====> Test set loss avg: 369.1040\n",
      "0.5192756652832031 seconds for epoch 31\n",
      "====> Epoch: 32 train set loss avg: 366.9199\n",
      "====> Test set loss avg: 368.3962\n",
      "0.5205404758453369 seconds for epoch 32\n",
      "====> Epoch: 33 train set loss avg: 366.3855\n",
      "====> Test set loss avg: 368.3660\n",
      "0.5189874172210693 seconds for epoch 33\n",
      "====> Epoch: 34 train set loss avg: 365.9259\n",
      "====> Test set loss avg: 366.2125\n",
      "0.528449296951294 seconds for epoch 34\n",
      "====> Epoch: 35 train set loss avg: 365.4095\n",
      "====> Test set loss avg: 365.8195\n",
      "0.5254557132720947 seconds for epoch 35\n",
      "====> Epoch: 36 train set loss avg: 364.8777\n",
      "====> Test set loss avg: 367.6743\n",
      "0.5269060134887695 seconds for epoch 36\n",
      "====> Epoch: 37 train set loss avg: 364.3330\n",
      "====> Test set loss avg: 365.2772\n",
      "0.5240631103515625 seconds for epoch 37\n",
      "====> Epoch: 38 train set loss avg: 364.0144\n",
      "====> Test set loss avg: 365.6251\n",
      "0.5379962921142578 seconds for epoch 38\n",
      "====> Epoch: 39 train set loss avg: 363.5502\n",
      "====> Test set loss avg: 365.6980\n",
      "0.5289747714996338 seconds for epoch 39\n",
      "====> Epoch: 40 train set loss avg: 363.0847\n",
      "====> Test set loss avg: 364.0120\n",
      "0.5211606025695801 seconds for epoch 40\n",
      "====> Epoch: 41 train set loss avg: 362.6300\n",
      "====> Test set loss avg: 363.6419\n",
      "0.5314137935638428 seconds for epoch 41\n",
      "====> Epoch: 42 train set loss avg: 362.1679\n",
      "====> Test set loss avg: 364.3567\n",
      "0.5287649631500244 seconds for epoch 42\n",
      "====> Epoch: 43 train set loss avg: 361.9034\n",
      "====> Test set loss avg: 363.8531\n",
      "0.5412235260009766 seconds for epoch 43\n",
      "====> Epoch: 44 train set loss avg: 361.6477\n",
      "====> Test set loss avg: 363.0343\n",
      "0.5545494556427002 seconds for epoch 44\n",
      "====> Epoch: 45 train set loss avg: 361.0588\n",
      "====> Test set loss avg: 361.9592\n",
      "0.6368622779846191 seconds for epoch 45\n",
      "====> Epoch: 46 train set loss avg: 360.7990\n",
      "====> Test set loss avg: 364.2240\n",
      "0.5735507011413574 seconds for epoch 46\n",
      "====> Epoch: 47 train set loss avg: 360.4035\n",
      "====> Test set loss avg: 361.7126\n",
      "0.5552866458892822 seconds for epoch 47\n",
      "====> Epoch: 48 train set loss avg: 359.9733\n",
      "====> Test set loss avg: 362.0196\n",
      "0.5423870086669922 seconds for epoch 48\n",
      "====> Epoch: 49 train set loss avg: 359.6899\n",
      "====> Test set loss avg: 361.7881\n",
      "0.5531771183013916 seconds for epoch 49\n",
      "====> Epoch: 50 train set loss avg: 359.3518\n",
      "====> Test set loss avg: 361.2803\n",
      "0.5586874485015869 seconds for epoch 50\n",
      "====> Epoch: 51 train set loss avg: 359.0038\n",
      "====> Test set loss avg: 360.6215\n",
      "0.554434061050415 seconds for epoch 51\n",
      "====> Epoch: 52 train set loss avg: 358.8210\n",
      "====> Test set loss avg: 361.2788\n",
      "0.557304859161377 seconds for epoch 52\n",
      "====> Epoch: 53 train set loss avg: 358.4948\n",
      "====> Test set loss avg: 359.4961\n",
      "0.5570003986358643 seconds for epoch 53\n",
      "====> Epoch: 54 train set loss avg: 358.1543\n",
      "====> Test set loss avg: 360.2436\n",
      "0.5408854484558105 seconds for epoch 54\n",
      "====> Epoch: 55 train set loss avg: 357.9107\n",
      "====> Test set loss avg: 360.2479\n",
      "0.5559647083282471 seconds for epoch 55\n",
      "====> Epoch: 56 train set loss avg: 357.4851\n",
      "====> Test set loss avg: 359.6357\n",
      "0.5572588443756104 seconds for epoch 56\n",
      "====> Epoch: 57 train set loss avg: 357.3668\n",
      "====> Test set loss avg: 358.6944\n",
      "0.5635099411010742 seconds for epoch 57\n",
      "====> Epoch: 58 train set loss avg: 357.1081\n",
      "====> Test set loss avg: 359.1351\n",
      "0.5590007305145264 seconds for epoch 58\n",
      "====> Epoch: 59 train set loss avg: 356.8490\n",
      "====> Test set loss avg: 358.9104\n",
      "0.5613338947296143 seconds for epoch 59\n",
      "====> Epoch: 60 train set loss avg: 356.5534\n",
      "====> Test set loss avg: 359.1986\n",
      "0.5506925582885742 seconds for epoch 60\n",
      "====> Epoch: 61 train set loss avg: 356.4267\n",
      "====> Test set loss avg: 357.8761\n",
      "0.5500714778900146 seconds for epoch 61\n",
      "====> Epoch: 62 train set loss avg: 356.1866\n",
      "====> Test set loss avg: 358.2154\n",
      "0.5585012435913086 seconds for epoch 62\n",
      "====> Epoch: 63 train set loss avg: 355.9817\n",
      "====> Test set loss avg: 358.6521\n",
      "0.5550985336303711 seconds for epoch 63\n",
      "====> Epoch: 64 train set loss avg: 355.7258\n",
      "====> Test set loss avg: 357.5454\n",
      "0.569291353225708 seconds for epoch 64\n",
      "====> Epoch: 65 train set loss avg: 355.5304\n",
      "====> Test set loss avg: 358.2598\n",
      "0.5596504211425781 seconds for epoch 65\n",
      "====> Epoch: 66 train set loss avg: 355.3168\n",
      "====> Test set loss avg: 357.9538\n",
      "0.5641016960144043 seconds for epoch 66\n",
      "====> Epoch: 67 train set loss avg: 355.1408\n",
      "====> Test set loss avg: 357.4463\n",
      "0.5526635646820068 seconds for epoch 67\n",
      "====> Epoch: 68 train set loss avg: 354.9678\n",
      "====> Test set loss avg: 357.6764\n",
      "0.5736064910888672 seconds for epoch 68\n",
      "====> Epoch: 69 train set loss avg: 354.8381\n",
      "====> Test set loss avg: 357.2748\n",
      "0.5620286464691162 seconds for epoch 69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 70 train set loss avg: 354.5770\n",
      "====> Test set loss avg: 357.1261\n",
      "0.7017498016357422 seconds for epoch 70\n",
      "====> Epoch: 71 train set loss avg: 354.3965\n",
      "====> Test set loss avg: 356.0880\n",
      "0.6069254875183105 seconds for epoch 71\n",
      "====> Epoch: 72 train set loss avg: 354.1720\n",
      "====> Test set loss avg: 355.5947\n",
      "0.5505437850952148 seconds for epoch 72\n",
      "====> Epoch: 73 train set loss avg: 354.0570\n",
      "====> Test set loss avg: 356.4599\n",
      "0.5645318031311035 seconds for epoch 73\n",
      "====> Epoch: 74 train set loss avg: 353.9031\n",
      "====> Test set loss avg: 355.9669\n",
      "0.5556015968322754 seconds for epoch 74\n",
      "====> Epoch: 75 train set loss avg: 353.7036\n",
      "====> Test set loss avg: 355.1690\n",
      "0.5636999607086182 seconds for epoch 75\n",
      "====> Epoch: 76 train set loss avg: 353.5197\n",
      "====> Test set loss avg: 355.8105\n",
      "0.5658524036407471 seconds for epoch 76\n",
      "====> Epoch: 77 train set loss avg: 353.4280\n",
      "====> Test set loss avg: 355.6719\n",
      "0.5635559558868408 seconds for epoch 77\n",
      "====> Epoch: 78 train set loss avg: 353.2346\n",
      "====> Test set loss avg: 355.8447\n",
      "0.5688316822052002 seconds for epoch 78\n",
      "====> Epoch: 79 train set loss avg: 353.1217\n",
      "====> Test set loss avg: 355.2142\n",
      "0.5694980621337891 seconds for epoch 79\n"
     ]
    }
   ],
   "source": [
    "solver = Solver(model, data_loader, optimizer, z_dim, epochs, step_config, optim_config, warmup_epochs, beta, batch_size)\n",
    "solver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert name of model here if want to load a model, e.g. \"../models/VAE_MNIST_train_loss=151.39_z=2.pt\"\n",
    "#solver = torch.load(\"../models/VAE_MNIST_train_loss=97.15_z=20.pt\")\n",
    "#solver.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting train and test losses for all epochs\n",
    "plot_losses(solver, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gaussian_distributions(solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring the reconstruction loss (likelihood lower bound) and KL divergence\n",
    "DEBUG = 0\n",
    "if DEBUG:\n",
    "    for epoch, train_loss, test_loss, rl, kl in zip(solver.train_loss_history[\"epochs\"], \\\n",
    "                             solver.train_loss_history[\"train_loss_acc\"], \\\n",
    "                             solver.test_loss_history, \\\n",
    "                             solver.train_loss_history[\"recon_loss_acc\"], \\\n",
    "                             solver.train_loss_history[\"kl_diverg_acc\"]):\n",
    "        print(\"epoch: {}, train_loss: {:.2f}, test_loss: {:.2f}, recon. loss: {:.2f}, KL div.: {:.2f}\".format(\n",
    "            epoch, train_loss, test_loss, rl, kl))\n",
    "        print(\"overfitting: {:.2f}\".format(abs(test_loss-train_loss)))\n",
    "plot_rl_kl(solver, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize q(z) (latent space z)\n",
    "if solver.z_dim == 2:\n",
    "    if solver.loader.dataset == \"FF\":\n",
    "        plot_latent_space_no_labels(solver)\n",
    "    else:\n",
    "        plot_latent_space(solver)\n",
    "else:\n",
    "    print(\"Plot of latent space not possible as dimension of z is not 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations of learned data manifold for generative models with two-dimensional latent space\n",
    "if solver.z_dim == 2:\n",
    "    if solver.loader.dataset == \"MNIST\":\n",
    "        plot_latent_manifold(solver, \"bone\")\n",
    "    if solver.loader.dataset == \"LFW\" or solver.loader.dataset == \"FF\":\n",
    "        plot_latent_manifold(solver, \"gray\", n=10, fig_size=(10, 8))\n",
    "else:\n",
    "    print(\"Plot is not possible as dimension of z is not 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots real faces and in grid samples\n",
    "if solver.loader.dataset == \"LFW\" or solver.loader.dataset == \"FF\":\n",
    "    plot_faces_grid(225, 15, solver)\n",
    "    plot_faces_samples_grid(225, 15, solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_loss = solver.train_loss_history[\"train_loss_acc\"][-1]\n",
    "torch.save(solver, \"../models/VAE_\" + solver.loader.dataset + \"_train_loss=\" + \"{0:.2f}\".format(last_train_loss) + \"_z=\" + str(solver.z_dim) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
